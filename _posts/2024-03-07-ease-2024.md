---
layout: post
title: "\"Using Large Language Models to Generate JUnit Tests: An Empirical Study\" accepted at EASE 2024."
short_title: "Paper accepted at EASE 2024"
date: 2024-03-07 12:00:00 -0400
categories: paper research LLM
author: "Mohammed Latif Siddiq"
img: EASE2024.png
thumb: EASE2024.png
tags: ease-24
paper_id: ease-2024
excerpt: "Our paper, \"Using Large Language Models to Generate JUnit Tests: An Empirical Study\", got accepted for the 28th International Conference on Evaluation and Assessment in Software Engineering (EASE 2024) in the research track."
---

Our paper, **Using Large Language Models to Generate JUnit Tests: An Empirical Study**, got accepted for the 28th International Conference on Evaluation and Assessment in Software Engineering (EASE 2024) in the research track. In this work, we analyzed three models with different prompting techniques to generate JUnit tests for the HumanEval dataset and real-world software. We evaluated the LLMs' generated tests using compilation rates, test correctness, test coverage, and test smell. We found that though the models have higher coverage for small programming problems from the HumanEval dataset, they lack good coverage for real-world software from the Evosuite dataset.
