---
layout: post
title: "\"Large Language Models in Computer Science Education: A Systematic Literature Review\" accepted at SIGCSE TS 2025"
short_title: "LLM on CS education survey accepted at SIGCSE TS 2025"
date: 2024-01-23 12:00:00 -0400
categories: paper research LLM
author: "Joanna C. S. Santos"
img: naacl-2025.png
thumb: acl-logo.png
tags: naacl
paper_id: naacl-2025
excerpt: "Full research paper got accepted at the findings of the 2025 Annual Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics (NAACL 2025 Findings) ."
---

Our paper, "MojoBench: Language Modeling and Benchmarks for Mojo", has been accepted to NAACL 2025 Findings.

In this paper, we introduce MojoBench, the first framework for Mojo code generation. MojoBench includes HumanEval-Mojo, a benchmark dataset designed for evaluating code LLMs on Mojo, and Mojo-Coder, the first LLM pretrained and finetuned for Mojo code generation, which supports instructions in 5 natural languages (NLs). Our results show that Mojo-Coder achieves a 30-35% performance improvement over leading models like GPT-4o and Claude-3.5-Sonnet. Furthermore, we provide insights into LLM behavior with underrepresented and unseen PLs, offering potential strategies for enhancing model adaptability. MojoBench contributes to our understanding of LLM capabilities and limitations in emerging programming paradigms fostering more robust code generation systems. 



    